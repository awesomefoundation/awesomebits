{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb0fd4e",
   "metadata": {},
   "source": [
    "# 03 - Model Evaluation\n",
    "## Batch Results, Model Comparison, and Cross-Chapter Analysis\n",
    "\n",
    "Analyze LLM scoring results from the Anthropic Batch API. Compare models, measure test-retest reliability, and validate across chapters.\n",
    "\n",
    "**Data source:** Parameterized via `AWESOMEBITS_DB` env var.\n",
    "\n",
    "**Score files:** Place batch result JSONs in `.scratch/data/` with names like `scores-haiku-run1.json`, `scores-sonnet-run1.json`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661387a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, json, glob\n",
    "sys.path.insert(0, '.')\n",
    "from helpers import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "setup_plotting()\n",
    "con = connect()\n",
    "data_dir = Path(DB_PATH).parent\n",
    "print(f'Data directory: {data_dir}')\n",
    "print(f'Score files found:')\n",
    "for f in sorted(data_dir.glob('scores-*.json')):\n",
    "    with open(f) as fh:\n",
    "        d = json.load(fh)\n",
    "    print(f'  {f.name}: {len(d)} scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc822e",
   "metadata": {},
   "source": [
    "## Load Scores\n",
    "\n",
    "Load all available score files and merge with project metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4607f16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scores(pattern='scores-*.json'):\n",
    "    \"\"\"Load score files into a dict of DataFrames keyed by filename stem.\"\"\"\n",
    "    results = {}\n",
    "    for path in sorted(data_dir.glob(pattern)):\n",
    "        with open(path) as f:\n",
    "            scores = json.load(f)\n",
    "        # Handle both list and dict formats\n",
    "        if isinstance(scores, dict):\n",
    "            rows = [{'project_id': int(k), **v} for k, v in scores.items()]\n",
    "        else:\n",
    "            rows = scores\n",
    "        sdf = pd.DataFrame(rows)\n",
    "        results[path.stem] = sdf\n",
    "    return results\n",
    "\n",
    "score_sets = load_scores()\n",
    "print(f'Loaded {len(score_sets)} score sets: {list(score_sets.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9fc8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge scores with labels from DuckDB\n",
    "projects = con.execute('''\n",
    "    SELECT p.id as project_id, p.title, p.funded_on, p.hidden_at,\n",
    "           c.name as chapter_name, c.country\n",
    "    FROM projects p\n",
    "    JOIN chapters c ON p.chapter_id = c.id\n",
    "''').df()\n",
    "projects['label'] = projects.apply(label_project, axis=1)\n",
    "\n",
    "# Merge each score set with labels\n",
    "for name, sdf in score_sets.items():\n",
    "    merged = sdf.merge(projects, on='project_id', how='left')\n",
    "    score_sets[name] = merged\n",
    "    labeled = merged[merged.label.isin(['funded', 'hidden'])]\n",
    "    print(f'{name}: {len(merged)} total, {len(labeled)} labeled '\n",
    "          f'(funded={len(labeled[labeled.label==\"funded\"])}, '\n",
    "          f'hidden={len(labeled[labeled.label==\"hidden\"])})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad8aed5",
   "metadata": {},
   "source": [
    "## Score Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef3ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composite score distribution per model\n",
    "score_col = 'composite_score'  # adjust if your JSON uses a different key\n",
    "# Try to find the right column name\n",
    "for name, sdf in score_sets.items():\n",
    "    candidates = [c for c in sdf.columns if 'score' in c.lower() and 'composite' in c.lower()]\n",
    "    if not candidates:\n",
    "        candidates = [c for c in sdf.columns if c in ('composite_score', 'score', 'total_score')]\n",
    "    if candidates:\n",
    "        score_col = candidates[0]\n",
    "        break\n",
    "print(f'Using score column: {score_col}')\n",
    "\n",
    "fig, axes = plt.subplots(1, len(score_sets), figsize=(6 * len(score_sets), 5), squeeze=False)\n",
    "for ax, (name, sdf) in zip(axes[0], score_sets.items()):\n",
    "    if score_col not in sdf.columns:\n",
    "        ax.set_title(f'{name} (no {score_col})')\n",
    "        continue\n",
    "    for label, color in [('funded', 'mediumseagreen'), ('hidden', 'salmon'), ('unlabeled', 'lightgray')]:\n",
    "        subset = sdf[sdf.label == label][score_col].dropna()\n",
    "        if len(subset):\n",
    "            ax.hist(subset, bins=20, alpha=0.6, color=color, label=f'{label} (n={len(subset)})', range=(0, 1))\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('Composite Score')\n",
    "    ax.legend()\n",
    "plt.suptitle('Score Distributions by Model and Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b3810",
   "metadata": {},
   "source": [
    "## Classification Metrics\n",
    "\n",
    "Using composite score thresholds, how well does each model separate funded from hidden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4dee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(sdf, score_col, threshold=0.5):\n",
    "    \"\"\"Compute classification metrics at a given threshold.\"\"\"\n",
    "    labeled = sdf[sdf.label.isin(['funded', 'hidden'])].dropna(subset=[score_col]).copy()\n",
    "    labeled['predicted'] = (labeled[score_col] >= threshold).map({True: 'funded', False: 'hidden'})\n",
    "\n",
    "    tp = ((labeled.label == 'funded') & (labeled.predicted == 'funded')).sum()\n",
    "    tn = ((labeled.label == 'hidden') & (labeled.predicted == 'hidden')).sum()\n",
    "    fp = ((labeled.label == 'hidden') & (labeled.predicted == 'funded')).sum()\n",
    "    fn = ((labeled.label == 'funded') & (labeled.predicted == 'hidden')).sum()\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = (tp + tn) / len(labeled) if len(labeled) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'n': len(labeled), 'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn,\n",
    "        'precision': round(precision, 3), 'recall': round(recall, 3),\n",
    "        'f1': round(f1, 3), 'accuracy': round(accuracy, 3),\n",
    "        'funded_avg': round(labeled[labeled.label == 'funded'][score_col].mean(), 3),\n",
    "        'hidden_avg': round(labeled[labeled.label == 'hidden'][score_col].mean(), 3),\n",
    "    }\n",
    "\n",
    "# Evaluate all models at multiple thresholds\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "for name, sdf in score_sets.items():\n",
    "    if score_col not in sdf.columns:\n",
    "        print(f'{name}: no {score_col} column')\n",
    "        continue\n",
    "    print(f'\\n=== {name} ===')\n",
    "    rows = []\n",
    "    for t in thresholds:\n",
    "        m = evaluate_model(sdf, score_col, threshold=t)\n",
    "        rows.append({'threshold': t, **m})\n",
    "    print(pd.DataFrame(rows)[['threshold', 'precision', 'recall', 'f1', 'accuracy', 'fn', 'fp']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f10549",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "If multiple models scored the same applications, compare them head-to-head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6536d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(score_sets) >= 2:\n",
    "    names = list(score_sets.keys())\n",
    "    # Merge first two score sets on project_id\n",
    "    a_name, b_name = names[0], names[1]\n",
    "    a = score_sets[a_name][['project_id', score_col, 'label']].rename(columns={score_col: f'{a_name}'})\n",
    "    b = score_sets[b_name][['project_id', score_col]].rename(columns={score_col: f'{b_name}'})\n",
    "    comp = a.merge(b, on='project_id', how='inner')\n",
    "\n",
    "    print(f'Comparing {a_name} vs {b_name}: {len(comp)} shared applications')\n",
    "\n",
    "    # Correlation\n",
    "    r, p = stats.pearsonr(comp[a_name].dropna(), comp[b_name].dropna())\n",
    "    rho, _ = stats.spearmanr(comp[a_name].dropna(), comp[b_name].dropna())\n",
    "    print(f'Pearson r={r:.3f} (p={p:.2e})')\n",
    "    print(f'Spearman rho={rho:.3f}')\n",
    "\n",
    "    # Score difference stats\n",
    "    comp['diff'] = comp[a_name] - comp[b_name]\n",
    "    print(f'Mean diff: {comp[\"diff\"].mean():.3f}')\n",
    "    print(f'Std diff: {comp[\"diff\"].std():.3f}')\n",
    "    print(f'Max abs diff: {comp[\"diff\"].abs().max():.3f}')\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    colors = comp.label.map({'funded': 'mediumseagreen', 'hidden': 'salmon', 'unlabeled': 'lightgray'})\n",
    "    ax1.scatter(comp[a_name], comp[b_name], alpha=0.4, c=colors, s=20)\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "    ax1.set_xlabel(a_name)\n",
    "    ax1.set_ylabel(b_name)\n",
    "    ax1.set_title(f'Score Comparison (r={r:.3f})')\n",
    "\n",
    "    ax2.hist(comp['diff'], bins=40, color='steelblue', alpha=0.7)\n",
    "    ax2.axvline(0, color='black', linestyle='--', alpha=0.3)\n",
    "    ax2.set_xlabel('Score Difference')\n",
    "    ax2.set_title(f'Score Difference Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Only one score set available. Submit additional batches for comparison.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baad6c12",
   "metadata": {},
   "source": [
    "## Test-Retest Reliability\n",
    "\n",
    "If the same model was run twice on the same inputs, measure score stability. Look for score files like `scores-haiku-run1.json` and `scores-haiku-run2.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6627ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find run pairs (same model, different runs)\n",
    "import re as _re\n",
    "run_pairs = {}\n",
    "for name in score_sets:\n",
    "    m = _re.match(r'(scores-\\w+)-run(\\d+)', name)\n",
    "    if m:\n",
    "        base, run_num = m.group(1), int(m.group(2))\n",
    "        run_pairs.setdefault(base, {})[run_num] = name\n",
    "\n",
    "if run_pairs:\n",
    "    for base, runs in run_pairs.items():\n",
    "        if len(runs) < 2:\n",
    "            continue\n",
    "        r1_name = runs[min(runs)]\n",
    "        r2_name = runs[max(runs)]\n",
    "        r1 = score_sets[r1_name][['project_id', score_col]].rename(columns={score_col: 'run1'})\n",
    "        r2 = score_sets[r2_name][['project_id', score_col]].rename(columns={score_col: 'run2'})\n",
    "        paired = r1.merge(r2, on='project_id')\n",
    "\n",
    "        r, _ = stats.pearsonr(paired.run1, paired.run2)\n",
    "        rho, _ = stats.spearmanr(paired.run1, paired.run2)\n",
    "        diff = (paired.run1 - paired.run2).abs()\n",
    "\n",
    "        print(f'\\n=== {base} test-retest (n={len(paired)}) ===')\n",
    "        print(f'Pearson r={r:.4f}, Spearman rho={rho:.4f}')\n",
    "        print(f'Mean abs diff: {diff.mean():.4f}')\n",
    "        print(f'Max abs diff: {diff.max():.4f}')\n",
    "        print(f'Std diff: {(paired.run1 - paired.run2).std():.4f}')\n",
    "        # Threshold consistency\n",
    "        for t in [0.3, 0.5, 0.7]:\n",
    "            agree = ((paired.run1 >= t) == (paired.run2 >= t)).mean()\n",
    "            print(f'  Threshold {t}: {agree:.1%} agreement')\n",
    "else:\n",
    "    print('No test-retest pairs found. Run the same model twice and name files like:')\n",
    "    print('  scores-haiku-run1.json, scores-haiku-run2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef1db84",
   "metadata": {},
   "source": [
    "## Cross-Chapter Analysis\n",
    "\n",
    "Do scores generalize across chapters, or are patterns locality-specific?\n",
    "\n",
    "Chapters with enough labeled data for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c22898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show which chapters have enough data\n",
    "chapter_stats = con.execute('''\n",
    "    SELECT c.name, c.country, c.inactive_at IS NOT NULL as inactive,\n",
    "        COUNT(*) as total, COUNT(p.funded_on) as funded, COUNT(p.hidden_at) as hidden\n",
    "    FROM projects p JOIN chapters c ON p.chapter_id = c.id\n",
    "    GROUP BY c.name, c.country, inactive\n",
    "    HAVING COUNT(p.funded_on) >= 10 AND COUNT(p.hidden_at) >= 10\n",
    "    ORDER BY total DESC\n",
    "''').df()\n",
    "print(chapter_stats.to_string(index=False))\n",
    "print(f'\\n{len(chapter_stats)} chapters have enough labeled data for validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca55338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-chapter score analysis (if scores include chapter info)\n",
    "for name, sdf in score_sets.items():\n",
    "    if score_col not in sdf.columns or 'chapter_name' not in sdf.columns:\n",
    "        continue\n",
    "    print(f'\\n=== {name}: per-chapter scores ===')\n",
    "    ch_scores = sdf.groupby(['chapter_name', 'label'])[score_col].agg(['mean', 'count']).round(3)\n",
    "    ch_scores = ch_scores[ch_scores['count'] >= 5].reset_index()\n",
    "    # Pivot for readability\n",
    "    pivot = ch_scores.pivot_table(index='chapter_name', columns='label', values='mean')\n",
    "    if 'funded' in pivot.columns and 'hidden' in pivot.columns:\n",
    "        pivot['separation'] = (pivot['funded'] - pivot['hidden']).round(3)\n",
    "        print(pivot.sort_values('separation', ascending=False).to_string())\n",
    "    else:\n",
    "        print(pivot.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c481fa",
   "metadata": {},
   "source": [
    "## Predicted Quality Ratio\n",
    "\n",
    "Does the ~28% quality ratio (from Chicago labeled data) hold when scoring unlabeled applications?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70edee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, sdf in score_sets.items():\n",
    "    if score_col not in sdf.columns:\n",
    "        continue\n",
    "    print(f'\\n=== {name} ===')\n",
    "    for t in [0.3, 0.5, 0.7]:\n",
    "        unlabeled = sdf[sdf.label == 'unlabeled']\n",
    "        above = (unlabeled[score_col] >= t).mean()\n",
    "        print(f'  Threshold {t}: {above:.1%} of unlabeled apps score above')\n",
    "    # Overall distribution\n",
    "    print(f'  Unlabeled score stats: mean={sdf[sdf.label==\"unlabeled\"][score_col].mean():.3f}, '\n",
    "          f'median={sdf[sdf.label==\"unlabeled\"][score_col].median():.3f}')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
